{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 11\n",
    "\n",
    "Name:  Carlos Contreras\n",
    "UID:  U63425893\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Latent Semantic Analysis\n",
    "\n",
    "### Latent Semantic Analysis\n",
    "\n",
    "In this section we will fetch news articles from 3 different categories. We will perform Tfidf vectorization on the corpus of documents and use SVD to represent our corpus in the feature space of topics that we've uncovered from SVD. We will attempt to cluster the documents into 3 clusters as we vary the number of singular vectors we use to represent the corpus, and compare the output to the clustering created by the news article categories. Do we end up with a better clustering the more singular vectors we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m news_data \u001b[38;5;241m=\u001b[39m fetch_20newsgroups(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, categories\u001b[38;5;241m=\u001b[39mcategories)\n\u001b[0;32m     12\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m stemmed_data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(SnowballStemmer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, ignore_stopwords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstem(word)  \n\u001b[0;32m     15\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sent_tokenize(message)\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokenize(sent))\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m news_data\u001b[38;5;241m.\u001b[39mdata]\n\u001b[0;32m     19\u001b[0m dtm \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(stemmed_data)\n\u001b[0;32m     20\u001b[0m terms \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m news_data \u001b[38;5;241m=\u001b[39m fetch_20newsgroups(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, categories\u001b[38;5;241m=\u001b[39mcategories)\n\u001b[0;32m     12\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m stemmed_data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(SnowballStemmer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, ignore_stopwords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstem(word)  \n\u001b[0;32m     15\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sent_tokenize(message)\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokenize(sent))\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m news_data\u001b[38;5;241m.\u001b[39mdata]\n\u001b[0;32m     19\u001b[0m dtm \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(stemmed_data)\n\u001b[0;32m     20\u001b[0m terms \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m news_data \u001b[38;5;241m=\u001b[39m fetch_20newsgroups(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, categories\u001b[38;5;241m=\u001b[39mcategories)\n\u001b[0;32m     12\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m stemmed_data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(SnowballStemmer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, ignore_stopwords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstem(word)  \n\u001b[0;32m     15\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sent_tokenize(message)\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokenize(sent))\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m news_data\u001b[38;5;241m.\u001b[39mdata]\n\u001b[0;32m     19\u001b[0m dtm \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(stemmed_data)\n\u001b[0;32m     20\u001b[0m terms \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[1;32mc:\\Users\\Car_1\\anaconda3\\Lib\\site-packages\\nltk\\stem\\snowball.py:108\u001b[0m, in \u001b[0;36mSnowballStemmer.__init__\u001b[1;34m(self, language, ignore_stopwords)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe language \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m stemmerclass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[language\u001b[38;5;241m.\u001b[39mcapitalize() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStemmer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstemmer \u001b[38;5;241m=\u001b[39m stemmerclass(ignore_stopwords)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstemmer\u001b[38;5;241m.\u001b[39mstem\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstemmer\u001b[38;5;241m.\u001b[39mstopwords\n",
      "File \u001b[1;32mc:\\Users\\Car_1\\anaconda3\\Lib\\site-packages\\nltk\\stem\\snowball.py:140\u001b[0m, in \u001b[0;36m_LanguageSpecificStemmer.__init__\u001b[1;34m(self, ignore_stopwords)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_stopwords:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m stopwords\u001b[38;5;241m.\u001b[39mwords(language):\n\u001b[0;32m    141\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopwords\u001b[38;5;241m.\u001b[39madd(word)\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Car_1\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw(fileids))\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Car_1\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(f) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[38;5;241m.\u001b[39mappend(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32mc:\\Users\\Car_1\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root\u001b[38;5;241m.\u001b[39mjoin(file)\u001b[38;5;241m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32mc:\\Users\\Car_1\\anaconda3\\Lib\\site-packages\\nltk\\data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[0;32m    333\u001b[0m     _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FileSystemPathPointer(_path)\n",
      "File \u001b[1;32mc:\\Users\\Car_1\\anaconda3\\Lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args[\u001b[38;5;241m0\u001b[39m], add_py3_data(args[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m init_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Car_1\\anaconda3\\Lib\\site-packages\\nltk\\data.py:311\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03mCreate a new path pointer for the given absolute path.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m:raise IOError: If the given path does not exist.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    310\u001b[0m _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_path)\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_path):\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m _path\n",
      "File \u001b[1;32m<frozen genericpath>:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "categories = ['comp.os.ms-windows.misc', 'sci.space','rec.sport.baseball']\n",
    "news_data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=4,max_df=0.8)\n",
    "\n",
    "stemmed_data = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n",
    "         for sent in sent_tokenize(message)\n",
    "        for word in word_tokenize(sent))\n",
    "        for message in news_data.data]\n",
    "\n",
    "dtm = vectorizer.fit_transform(stemmed_data)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "centered_dtm = dtm - np.mean(dtm, axis=0)\n",
    "\n",
    "u, s, vt = np.linalg.svd(centered_dtm)\n",
    "plt.xlim([0,50])\n",
    "plt.plot(range(1,len(s)+1),s)\n",
    "plt.show()\n",
    "\n",
    "ag = []\n",
    "max = len(u)\n",
    "for singular_vectors in range(1,25):\n",
    "    vectorsk = u.dot(np.diag(s))[:,:singular_vectors]\n",
    "    kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=100, n_init=10, random_state=0)\n",
    "    kmeans.fit_predict(np.asarray(vectorsk))\n",
    "    labelsk = kmeans.labels_\n",
    "    ag.append(metrics.v_measure_score(labelsk, news_data.target)) # closer to 1 means closer to news categories\n",
    "\n",
    "plt.plot(range(1,25),ag)\n",
    "plt.ylabel('Agreement',size=20)\n",
    "plt.xlabel('No of Prin Comps',size=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "The data comes from the [Yelp Dataset](https://www.yelp.com/dataset). Each line is a review that consists of a label (0 for negative reviews and 1 for positive reviews) and a set of words.\n",
    "\n",
    "```\n",
    "1 i will never forget this single breakfast experience in mad...\n",
    "0 the search for decent chinese takeout in madison continues ...\n",
    "0 sorry but me julio fell way below the standard even for med...\n",
    "1 so this is the kind of food that will kill you so there s t...\n",
    "```\n",
    "\n",
    "In order to transform the set of words into vectors, we will rely on a method of feature engineering called word embeddings (Tfidf is one way to get these embeddings). Rather than simply indicating which words are present, word embeddings represent each word by \"embedding\" it in a low-dimensional vector space which may carry more information about the semantic meaning of the word. (for example in this space, the words \"King\" and \"Queen\" would be close).\n",
    "\n",
    "`word2vec.txt` contains the `word2vec` embeddings for about 15 thousand words. Not every word in each review is present in the provided `word2vec.txt` file. We can treat these words as being \"out of vocabulary\" and ignore them.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let x_i denote the sentence `“a hot dog is not a sandwich because it is not square”` and let a toy word2vec dictionary be as follows:\n",
    "\n",
    "```\n",
    "hot      0.1     0.2     0.3\n",
    "not      -0.1    0.2     -0.3\n",
    "sandwich 0.0     -0.2    0.4\n",
    "square   0.2     -0.1    0.5\n",
    "```\n",
    "\n",
    "we would first `trim` the sentence to only contain words in our vocabulary: `\"hot not sandwich not square”` then embed x_i into the feature space:\n",
    "\n",
    "$$ φ2(x_i)) = \\frac{1}{5} (word2vec(\\text{hot}) + 2 · word2vec(\\text{not}) + word2vec(\\text{sandwich}) + word2vec(\\text{square})) = \\left[0.02 \\hspace{2mm} 0.06 \\hspace{2mm} 0.12 \\hspace{2mm}\\right]^T $$\n",
    "\n",
    "a) Implement a function to trim out-of-vocabulary words from the reviews. Your function should return an nd array of the same dimension and dtype as the original loaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "VECTOR_LEN = 300   # Length of word2vec vector\n",
    "MAX_WORD_LEN = 64  # Max word length in dict.txt and word2vec.txt\n",
    "\n",
    "def load_tsv_dataset(file):\n",
    "    \"\"\"\n",
    "    Loads raw data and returns a tuple containing the reviews and their ratings.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): File path to the dataset tsv file.\n",
    "\n",
    "    Returns:\n",
    "        An np.ndarray of shape N. N is the number of data points in the tsv file.\n",
    "        Each element dataset[i] is a tuple (label, review), where the label is\n",
    "        an integer (0 or 1) and the review is a string.\n",
    "    \"\"\"\n",
    "    dataset = np.loadtxt(file, delimiter='\\t', comments=None, encoding='utf-8',\n",
    "                         dtype='l,O')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_feature_dictionary(file):\n",
    "    \"\"\"\n",
    "    Creates a map of words to vectors using the file that has the word2vec\n",
    "    embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): File path to the word2vec embedding file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary indexed by words, returning the corresponding word2vec\n",
    "        embedding np.ndarray.\n",
    "    \"\"\"\n",
    "    word2vec_map = dict() #\n",
    "    with open(file) as f:\n",
    "        read_file = csv.reader(f, delimiter='\\t')\n",
    "        for row in read_file:\n",
    "            word, embedding = row[0], row[1:]\n",
    "            word2vec_map[word] = np.array(embedding, dtype=float)\n",
    "    return word2vec_map\n",
    "\n",
    "\n",
    "def trim_reviews(path_to_dataset):\n",
    "\n",
    "    #loading the dataset\n",
    "    data = load_tsv_dataset(path_to_dataset)\n",
    "\n",
    "    #loading the embeddings word2vec\n",
    "    word2vec_map = load_feature_dictionary(r\"C:\\Users\\Car_1\\OneDrive\\Desktop\\CS_Classes\\cs506\\Data-Science-Fundamentals\\lecture_11\\data\\word2vec.txt\")\n",
    "\n",
    "    #trimming reviews\n",
    "\n",
    "    # making an empty array to store results\n",
    "    trimmed = []\n",
    "\n",
    "    # going over each tuple in the dataset to trim words not present in the word2vec dictionary\n",
    "    for label, review in data:\n",
    "\n",
    "        trim_review = ' '.join(word for word in review.split() if word in word2vec_map) #trimming the review\n",
    "        trimmed.append((label, trim_review)) #adding the trimmed review to the array\n",
    "\n",
    "    #creating a numpyt array of the trimmed reviews \n",
    "    trimmed_final = np.array(trimmed,dtype='l,O')\n",
    "\n",
    "    return trimmed_final\n",
    "\n",
    "trim_train = trim_reviews(r\"C:\\Users\\Car_1\\OneDrive\\Desktop\\CS_Classes\\cs506\\Data-Science-Fundamentals\\lecture_11\\data\\train_small.tsv\")\n",
    "trim_test = trim_reviews(r\"C:\\Users\\Car_1\\OneDrive\\Desktop\\CS_Classes\\cs506\\Data-Science-Fundamentals\\lecture_11\\data\\test_small.tsv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
